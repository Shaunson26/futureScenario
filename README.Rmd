---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# futureScenario

<!-- badges: start -->
<!-- badges: end -->

The goal of futureScenario is to ...

## to do

1. Obtain and create useable GNAF data - **done**, [see vignettes](vignettes/)
    - revisit this, some addresses are missing
    - lat/lon as integer to shrink file size
2. API calls datasets.seed.nsw.gov.au, data-cbr.csiro.au, ...
3. Obtain and aggregate data
4. Create plumber API
5. Generate reporting outputs  
6. Generate shiny apps

## Installation

<!--
You can install the released version of futureScenario from [CRAN](https://CRAN.R-project.org) with:

``` r
install.packages("futureScenario")
```
-->

And the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("Shaunson26/futureScenario")
```

```{r, eval=F}
library(futureScenario)
```

```{r, include=FALSE}
library(devtools)
load_all()

options(width = 250)
```

## Data

A flattened and minified GNAF address dataset with meshblock 2016 boundary `?gnaf`

```{r}
head(gnaf)
```

## Functions

`return_address_coords()` is used to query `gnaf`. Currently with exact matches only.
If any query files are missing, there will be a wildcard used for that those fields

```{r}
# An exact match
return_address_coords(street_number = 2, street_name = 'IVY STREET', locality = 'DARLINGTON', postcode = '2008')

# missing street_name returns all street_names matching the rest of the query
return_address_coords(street_number = 2, locality = 'DARLINGTON', postcode = '2008')
```

## API calls

### data-cbr.csiro.au

https://www.climatechangeinaustralia.gov.au/en/obtain-data/download-datasets/   

Huge raster data are available using NetcdfSubset REST API. Datasets are split by:

- variable  
- year range  
- model  
- greenhouse gas scenario (representative concentration pathways, RCP)  

Within each dataset, data selection parameters include:  

- variable
- lat/lon  
- date start/end  
- date step  

The object `csiro_catalog (R/csiro_catalog.R)` is a named list to help select the dataset for URL building

```{r}
csiro_catalog$variable
```

```{r}
csiro_catalog$year_range
```

```{r}
csiro_catalog$model
```

```{r}
csiro_catalog$rcp
```

```{r}
csiro_catalog$filename
```

We can build the queries using `create_dataset_url()` which usesv `csiro_catalog`, some hardcoded URLs (wihtin the function) and the package `httr2`.

```{r}
create_dataset_url(variable = csiro_catalog$variable$`Rainfall_(Precipitation)`,
                   model = csiro_catalog$model$`NorESM1-M`,
                   rcp = csiro_catalog$rcp$rcp45,
                   year_range = csiro_catalog$year_range$`2016-2045`)
```

This function is used within `download_netcdf_subset()` along with API query parameters to
download a dataset netcdf4 file to a temporary location(the path is returned by the function, 
`{randomChars}_{variable}_{model}_{rcp}_{date_range}.nc`). Of note is the coordinates requested:
either `lat`/`lon` or a bounding box `bbox` can be used. A NSW bounding box `nsw_bbox` is shipped 
with the package. Also two download methods exist - using `download.file()` or `writeBin(body)` 
from `httr2`. The former is quicker, but seems to fail often. `httr2` methods are more 
polite? Or the CSIRO server is under load when running these?

```{r, eval = F}
# Get lat/lon
addr <-
  return_address_coords(street_number = 2, street_name = 'IVY STREET', locality = 'DARLINGTON', postcode = '2008')

# download file
downloaded_file_path <-
  download_netcdf_subset(variable = csiro_catalog$variable$`Rainfall_(Precipitation)`,
                         model = csiro_catalog$model$`NorESM1-M`,
                         rcp = csiro_catalog$rcp$rcp85,
                         year_range = csiro_catalog$year_range$`2016-2045`,
                         lat = addr$LATITUDE, lon = addr$LONGITUDE,
                         #bbox = nsw_bbox,
                         date_start = '2016-01-01', date_end = '2016-01-03',
                         date_step = 2,
                         method = 'httr2')

raster_data <- stars::read_ncdf(downloaded_file_path, var = get_var_from_path(downloaded_file_path))

raster_data

raster_data %>% 
  tibble::as_tibble()
```

#### Example bulk download

Create a function with set parameters for model, rcp, years, dates and vary the variable parameter.

```{r, eval = F}
bulk_download <- function(x, address_df){
  download_netcdf_subset(variable = x,
                         model = csiro_catalog$model$`NorESM1-M`,
                         rcp = csiro_catalog$rcp$rcp85,
                         year_range = csiro_catalog$year_range$`2016-2045`,
                         lat = address_df$LATITUDE, lon = address_df$LONGITUDE,
                         date_start = '2016-01-01', date_end = '2045-12-31',
                         date_step = 365, method = 'httr2')
}

address <-
  return_address_coords(locality = 'PENRITH') %>% 
  dplyr::slice(sample(dplyr::n(), 1))

# Download separately in case of connection errors
rsds <- 
  bulk_download(csiro_catalog$variable$Solar_Radiation, 
                address_df = address)
hurs <- 
  bulk_download(csiro_catalog$variable$Relative_Humidity, 
                address_df = address)
pr <- 
  bulk_download(csiro_catalog$variable$`Rainfall_(Precipitation)`, 
                address_df = address)
tasmin <- 
  bulk_download(csiro_catalog$variable$Minimum_Temperature, 
                address_df = address)
tas <- 
  bulk_download(csiro_catalog$variable$Mean_Temperature, 
                address_df = address)
tasmax <- 
  bulk_download(csiro_catalog$variable$Maximum_Temperature, 
                address_df = address)
wvap <- 
  bulk_download(csiro_catalog$variable$Evaporation, 
                address_df = address)

library(stars)
library(dplyr)
library(ggplot2)

stars_list <-
  list(hurs, pr, tasmin, tas, tasmax, wvap) %>%
  lapply(., function(x){
    # extract variable from filename
    var = strsplit(basename(x), split = '_')[[1]][2]
    # import
    stars::read_ncdf(x, var = var)
  })

# one dataset has weird time class
class_PCICt <- function(x){
  class(x)[1] == 'PCICt'
}

purrr::map_df(zz, function(x){
  x %>%
    as_tibble() %>%
    select(everything(), value = 4) %>%
    mutate(across(where(class_PCICt), as.character),
           time = as.Date(time),
           value = as.numeric(value),
           var = names(x)) %>%
    # may get multiple grids for a give lat/lon so average to 1 value
    group_by(time) %>%
    summarise(value = mean(value)) %>%
    ungroup()
})
```


### Heat vulnerability

call to https://datasets.seed.nsw.gov.au/dataset/...

```{r}
hvi <-
  return_address_coords(street_number = 2, street_name = 'IVY STREET', locality = 'DARLINGTON', postcode = '2008') %>%
  join_sa1() %>%
  dplyr::pull(SA1_MAINCODE_2016) %>%
  get_heat_vulnerability_index(sa1 = .) %>%  # JSON as a list
  map_heat_vulnerability_index()

hvi %>% 
  do.call(rbind.data.frame, .)
```

### Urban vegetation cover

call to https://datasets.seed.nsw.gov.au/dataset/...

```{r}
uvca <-
  return_address_coords(street_number = 2, street_name = 'IVY STREET', locality = 'DARLINGTON', postcode = '2008') %>%
  dplyr::pull(MB_2016_CODE) %>%
  get_urban_vegetation_cover_all(mb = .) %>%  # JSON as a list
  map_urban_vegetation_cover_all()

uvca
```

## Shiny

Shiny apps for exploration and other purposes

```{r, eval = F}
run_gnaf_leaflet_shiny()
```

![Shiny example gif](github-files/run_gnaf_leaflet_shiny_small.gif)


